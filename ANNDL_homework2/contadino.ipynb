{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7432b89e811a42160cd3b01cd2cc10cd95c4849b856700297acfb5893f1a7106"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "# costants\n",
    "SEED = 0xDED\n",
    "KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "if KAGGLE:\n",
    "    print(\"Detected kaggle environment\")\n",
    "\n",
    "target_w = 512\n",
    "target_h = 512\n",
    "\n",
    "batch_size = 1\n",
    "num_classes = 3    \n",
    "epochs = 1\n",
    "\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialization and common utilities\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def fromCwd(path):\n",
    "    return os.path.join(os.getcwd(), path)\n",
    "\n",
    "\n",
    "if KAGGLE:\n",
    "    train_dir = \"/kaggle/temp/training\"\n",
    "    valid_dir = \"/kaggle/temp/validation\"\n",
    "    data_dir = \"/kaggle/input/contadinoset/Development_Dataset/Training\"\n",
    "    temp_dir = \"/kaggle/temp/mvdir\"\n",
    "    checkpoint_dir = \"/kaggle/temp/checkpoints\"\n",
    "    test_dir = \"/kaggle/input/contadinoset/Development_Dataset/Test_Dev\"\n",
    "    output_dir = fromCwd(\"/kaggle/working\")\n",
    "else:\n",
    "    train_dir = fromCwd(\"training\")\n",
    "    valid_dir = fromCwd(\"validation\")\n",
    "    data_dir = fromCwd(\"data\")\n",
    "    temp_dir = fromCwd(\"temp\")\n",
    "    checkpoint_dir = fromCwd(\"checkpoints\")\n",
    "    test_dir = fromCwd(\"test\")\n",
    "    output_dir = fromCwd(\"output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# move all data to images or mask folders into dest dir\n",
    "def moveRecursive(source_dir, folders, percent):\n",
    "\n",
    "    items = os.listdir(source_dir)\n",
    "    print(\"Moving from \"+ source_dir)\n",
    "\n",
    "    if not \"Images\" in items: # normal top folder we should explore all its sub folders\n",
    "        for fold in items:\n",
    "            moveRecursive(os.path.join(source_dir, fold), folders, percent)\n",
    "        return # no more to do in this folder\n",
    "\n",
    "    # here we got Images in subfolder\n",
    "    im_dir = os.path.join(source_dir, \"Images\")\n",
    "    ms_dir = os.path.join(source_dir, \"Masks\")\n",
    "    # read all images and masks\n",
    "    images = os.listdir(im_dir)\n",
    "    masks = os.listdir(ms_dir)\n",
    "\n",
    "    # make validation\n",
    "    val_sz = int(len(images) * percent) + 1\n",
    "    for i in range(val_sz):\n",
    "        idx = np.random.randint(0, len(images))\n",
    "\n",
    "        # move both image and mask to validation folder\n",
    "        os.rename(os.path.join(im_dir, images[idx]), os.path.join(folders[2], images[idx]))\n",
    "        os.rename(os.path.join(ms_dir, masks[idx]), os.path.join(folders[3], masks[idx]))\n",
    "\n",
    "        # update refreshed folders without moved elements\n",
    "        images = os.listdir(im_dir)\n",
    "        masks = os.listdir(ms_dir)\n",
    "\n",
    "    # create train set\n",
    "    for img in images:\n",
    "        os.rename(os.path.join(im_dir, img), os.path.join(folders[0], img))\n",
    "    for msk in masks:\n",
    "        os.rename(os.path.join(ms_dir, msk), os.path.join(folders[1], msk))\n",
    "\n",
    "#end    \n",
    "\n",
    "\n",
    "# move all data from subfolders in source folder to Images/Mask folders in dest folder\n",
    "def prepareTrainData(source_dir, train_dir, valid_dir, percent = 0.1):\n",
    "\n",
    "    folders = [] # order: Test_img | Test_mask | valid_img | valid_mask\n",
    "    for parent in [train_dir, valid_dir]:\n",
    "        for sub in [\"Images\", \"Masks\"]:\n",
    "            folders.append(os.path.join(parent,sub))\n",
    "\n",
    "    # check if a folder already exist and skip \n",
    "    for f in folders:\n",
    "        if os.path.exists(f):\n",
    "            print(\"[Skipped] Directory already exists: \"+ f)\n",
    "            return folders\n",
    "\n",
    "    # create folders\n",
    "    for f in folders:   \n",
    "        os.makedirs(f)\n",
    "\n",
    "    moveRecursive(source_dir, folders, percent)\n",
    "    print(\"Train and Valid set ready\")\n",
    "\n",
    "    return folders\n",
    "\n",
    "if not os.path.exists(temp_dir):\n",
    "    shutil.copytree(data_dir, temp_dir) # required to copy everythin because preparaTrainData move images\n",
    "\n",
    "folders = prepareTrainData(temp_dir, train_dir, valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def printGrid(sub_im):\n",
    "    plt.figure(figsize=(27, 27))\n",
    "    for i in range(len(sub_im)):\n",
    "        ax = plt.subplot(IMG_DIVIDER, IMG_DIVIDER ,i+1)\n",
    "        \n",
    "        plt.imshow(np.uint8(sub_im[i]))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    plt.plot()\n",
    "\n",
    "\n",
    "#printGrid(sub_images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create generators\n",
    "rot_range = 45\n",
    "h_shift = 10\n",
    "w_shift = 10\n",
    "h_flip = True\n",
    "w_flip = True\n",
    "zoom = 0.3\n",
    "\n",
    "img_data_gen = ImageDataGenerator(\n",
    "    rotation_range=rot_range,\n",
    "    width_shift_range=w_shift,\n",
    "    height_shift_range=h_shift,\n",
    "    zoom_range=zoom,\n",
    "    horizontal_flip=h_flip,\n",
    "    vertical_flip=w_flip,\n",
    "    fill_mode='reflect'\n",
    "    )\n",
    "\n",
    "mask_data_gen = ImageDataGenerator(\n",
    "    rotation_range=rot_range,\n",
    "    width_shift_range=w_shift,\n",
    "    height_shift_range=h_shift,\n",
    "    zoom_range=zoom,\n",
    "    horizontal_flip=h_flip,\n",
    "    vertical_flip=w_flip,\n",
    "    fill_mode='reflect'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #convert a mask in rgb to actual classes\n",
    "  def rgb_to_class(mask_arr):\n",
    "\n",
    "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n",
    "\n",
    "    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n",
    "    new_mask_arr[np.where(np.all(mask_arr == [216, 124, 18], axis=-1))] = 0\n",
    "    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n",
    "    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n",
    "\n",
    "    return new_mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom generator\n",
    "\n",
    "class CustomGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "  def __init__(self, directory, img_generator=None, mask_generator=None, preprocessing_function=None, out_shape=[256, 256]):\n",
    "    self.img_generator = img_generator\n",
    "    self.mask_generator = mask_generator\n",
    "    self.preprocessing_function = preprocessing_function\n",
    "    self.out_shape = out_shape\n",
    "    self.im_path = os.path.join(directory, \"Images\")\n",
    "    self.ms_path = os.path.join(directory, \"Masks\")\n",
    "    self.images = np.sort(os.listdir(self.im_path)) \n",
    "    self.masks = np.sort(os.listdir(self.ms_path)) \n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # Read Image\n",
    "    img = Image.open(os.path.join(self.im_path, self.images[index]))\n",
    "    mask = Image.open(os.path.join(self.ms_path, self.masks[index]))\n",
    "\n",
    "    # Resize image and mask\n",
    "    img = img.resize(self.out_shape)\n",
    "    mask = mask.resize(self.out_shape, resample=Image.NEAREST)\n",
    "    \n",
    "    img_arr = np.array(img) / 255.\n",
    "    mask_arr = np.array(mask)\n",
    "\n",
    "    out_mask = []\n",
    "\n",
    "    if self.img_generator is not None and self.mask_generator is not None:\n",
    "        # Perform data augmentation\n",
    "        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n",
    "        mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n",
    "        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n",
    "        # ImageDataGenerator use bilinear interpolation for augmenting the images.\n",
    "        # Thus, when applied to the masks it will output 'interpolated classes', which\n",
    "        # is an unwanted behaviour. As a trick, we can transform each class mask \n",
    "        # separately and then we can cast to integer values (as in the binary segmentation notebook).\n",
    "        # Finally, we merge the augmented binary masks to obtain the final segmentation mask.\n",
    "        out_mask = np.zeros_like(mask_arr)\n",
    "        for c in np.unique(mask_arr):\n",
    "          if c > 0:\n",
    "            curr_class_arr = np.float32(mask_arr == c)\n",
    "            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n",
    "            # from [0, 1] to {0, 1}\n",
    "            curr_class_arr = np.uint8(curr_class_arr)\n",
    "            # recover original class\n",
    "            curr_class_arr = curr_class_arr * c \n",
    "            out_mask += curr_class_arr\n",
    "    else:\n",
    "      out_mask = mask_arr\n",
    "    \n",
    "    if self.preprocessing_function is not None:\n",
    "        img_arr = self.preprocessing_function(img_arr)\n",
    "\n",
    "    out_mask = rgb_to_class(np.float32(out_mask))\n",
    "    out_mask = np.expand_dims(out_mask, -1)\n",
    "\n",
    "\n",
    "    return img_arr, out_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = CustomGenerator(train_dir,\n",
    "                        img_generator=img_data_gen, \n",
    "                        mask_generator=mask_data_gen,\n",
    "                        out_shape=[target_h, target_w]\n",
    "                        )\n",
    "\n",
    "valid = CustomGenerator(valid_dir, out_shape=[target_h, target_w])\n",
    "\n",
    "print(\"Train dataset size: \"+ str(len(train)))\n",
    "print(\"Validation dataset size: \" + str(len(valid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pt = (tf.float32, tf.float32)\n",
    "out_shp = ([target_h, target_w, 3], [target_h, target_w, 1])\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train, output_types=out_pt, output_shapes=out_shp).batch(batch_size).repeat()\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: valid, output_types=out_pt, output_shapes=out_shp).batch(batch_size).repeat()\n",
    "\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Assign a color to each class\n",
    "evenly_spaced_interval = np.linspace(0, 1, 20)\n",
    "colors = [cm.rainbow(x) for x in evenly_spaced_interval]\n",
    "\n",
    "iterator = iter(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2):\n",
    "  # plot\n",
    "  fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "  augmented_img, target = next(iterator)\n",
    "  augmented_img = augmented_img[0]   # First element\n",
    "  augmented_img = augmented_img * 255  # denormalize\n",
    "\n",
    "\n",
    "  target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\n",
    "\n",
    "  print(np.unique(target))\n",
    "\n",
    "  target_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
    "\n",
    "  target_img[np.where(target == 0)] = [0, 0, 0]\n",
    "  target_img[np.where(target == 1)] = [0, 255, 0] \n",
    "  target_img[np.where(target == 2)] = [255, 0, 0] \n",
    "\n",
    "  ax[0].imshow(np.uint8(augmented_img))\n",
    "  ax[1].imshow(np.uint8(target_img))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "# input layer  \n",
    "\n",
    "inputs = keras.layers.Input(shape = [target_h, target_w,3])\n",
    "\n",
    "\n",
    "conv1 = keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "conv1 = keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "conv2 = keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "drop4 = keras.layers.Dropout(0.1)(conv4)\n",
    "pool4 = keras.layers.MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = keras.layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = keras.layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "drop5 = keras.layers.Dropout(0.1)(conv5)\n",
    "\n",
    "up6 = keras.layers.Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(keras.layers.UpSampling2D(size = (2,2))(drop5))\n",
    "merge6 = keras.layers.concatenate([drop4,up6], axis = 3)\n",
    "conv6 = keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "up7 = keras.layers.Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(keras.layers.UpSampling2D(size = (2,2))(conv6))\n",
    "merge7 = keras.layers.concatenate([conv3,up7], axis = 3)\n",
    "conv7 = keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "up8 = keras.layers.Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(keras.layers.UpSampling2D(size = (2,2))(conv7))\n",
    "merge8 = keras.layers.concatenate([conv2,up8], axis = 3)\n",
    "conv8 = keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "up9 = keras.layers.Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(keras.layers.UpSampling2D(size = (2,2))(conv8))\n",
    "merge9 = keras.layers.concatenate([conv1,up9], axis = 3)\n",
    "conv9 = keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv10 = keras.layers.Conv2D(num_classes, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "model = keras.models.Model(inputs = inputs, outputs = conv10)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanIoU(y_true, y_pred):\n",
    "    # get predicted class from softmax\n",
    "    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n",
    "\n",
    "    per_class_iou = []\n",
    "\n",
    "    for i in range(0,3): # exclude the background class 0\n",
    "      # Get prediction and target related to only a single class (i)\n",
    "      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n",
    "      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n",
    "      intersection = tf.reduce_sum(class_true * class_pred)\n",
    "      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n",
    "    \n",
    "      iou = (intersection + 1e-7) / (union + 1e-7)\n",
    "      per_class_iou.append(iou)\n",
    "\n",
    "    return tf.reduce_mean(per_class_iou)\n",
    "\n",
    "# compole model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='mse', \n",
    "    metrics=[meanIoU] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "#checkpoint_dir = checkpoint_dir+str(now)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "# setup callbacks\n",
    "# -----------------------\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath=os.path.join(checkpoint_dir, 'cp_{epoch:02d}.ckpt'), save_best_only=True, monitor='val_meanIoU'),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_meanIoU', patience=5)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "          x=train_dataset,\n",
    "          epochs=epochs,\n",
    "          steps_per_epoch=len(train)/batch_size,\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=len(valid)/batch_size,\n",
    "          callbacks=callbacks\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot alla metrics \n",
    "idx = 1\n",
    "mtr = ['loss','meanIoU']\n",
    "for m in mtr:\n",
    "    x = history.history[m]\n",
    "    val_x = history.history['val_' + m]\n",
    "\n",
    "    plt.subplot(1, len(mtr), idx)\n",
    "    plt.plot(x, label='Training ' + m)\n",
    "    plt.plot(val_x, label='Validation ' + m)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation ' + m)\n",
    "    idx += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(img):\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# return {rle crop prediction, rle weed prediction} img shape\n",
    "def make_prediction(model, image):\n",
    "\n",
    "    print(\"Original size: \"+ str(image.size))\n",
    "\n",
    "    img = image.resize([target_w, target_h]) # resize to the network expected size and keep a copy of the original img\n",
    "    img = np.array(img) / 255. # rescale\n",
    "    img = tf.expand_dims(img, 0) # add \"batch\" dimension\n",
    "\n",
    "    out = model(img, training=False) # predict\n",
    "\n",
    "    out= tf.squeeze(out, 0) # remove batch dimension to get a 3d vector\n",
    "    # we got a 3d vector with 3 deph and we need to rescale it to original input size\n",
    "    # so we denormalize if (output of softmax is 0 < val < 1)\n",
    "    # and we treat it as img to resize in plt\n",
    "    out = Image.fromarray(np.uint8(out * 255.)).resize(image.size)\n",
    "    out = np.array(out)\n",
    "    # rescale and resize should preserve the old max channels\n",
    "    # so we can ignore the rescaling to 0-1 again\n",
    "    out = tf.math.argmax(out, axis=-1) \n",
    "\n",
    "    print(\"Predictions:\" + str(out.shape))\n",
    "\n",
    "    return out, image.size\n",
    "\n",
    "# create function to evaluate the model and output results in requeisted format\n",
    "def output_predictions(model, test_dir):\n",
    "    submission_dict = {}\n",
    "    #print(test_dir)\n",
    "\n",
    "    # iterate all datasets keeping name\n",
    "    for team in os.listdir(test_dir): #team == dataset name\n",
    "        team_dir = os.path.join(test_dir, team)\n",
    "\n",
    "        # iterate all corps for a dataset keeping name\n",
    "        for crop in os.listdir(team_dir):\n",
    "            crop_dict = {}\n",
    "            img_dir = os.path.join(team_dir, crop)\n",
    "            img_dir = os.path.join(img_dir, \"Images\")\n",
    "            # iterate all images \n",
    "            for fl in os.listdir(img_dir):\n",
    "                #print(\"img: \"+ fl)\n",
    "                # get prediction from model\n",
    "                image = Image.open(os.path.join(img_dir, fl))\n",
    "                prediction, shape = make_prediction(model, image)\n",
    "\n",
    "                filename, file_extension = os.path.splitext(fl)\n",
    "                # prepare output dict\n",
    "                submission_dict[filename] = {\n",
    "                    'shape': shape,\n",
    "                    'team': team,\n",
    "                    'crop': crop,\n",
    "                    'segmentation': {'crop': rle_encode(prediction == 1), 'weed':rle_encode(prediction == 2)}\n",
    "                }\n",
    "            # end for fl\n",
    "        # end for crop\n",
    "    #end for team\n",
    "\n",
    "    return submission_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result of a validation image to understand if net is learning right\n",
    "\n",
    "if not KAGGLE:\n",
    "    iterator = iter(train_dataset)\n",
    "    img, mask = next(iterator)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(18, 5, forward=True)\n",
    "\n",
    "    img = img[0] * 255\n",
    "    i = Image.fromarray(np.uint8(img))\n",
    "    out, shape = make_prediction(model, i)\n",
    "\n",
    "    # expected mask\n",
    "    target = np.array(mask[0, ..., 0])   # First element (squeezing channel dimension)\n",
    "\n",
    "    target_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
    "\n",
    "    target_img[np.where(target == 0)] = [0, 0, 0]\n",
    "    target_img[np.where(target == 1)] = [0, 255, 0] \n",
    "    target_img[np.where(target == 2)] = [255, 0, 0] \n",
    "\n",
    "    #prediction\n",
    "    out_img = np.zeros([out.shape[0], out.shape[1], 3])\n",
    "\n",
    "    out_img[np.where(out == 0)] = [0, 0, 0]\n",
    "    out_img[np.where(out == 1)] = [0, 255, 0] \n",
    "    out_img[np.where(out == 2)] = [255, 0, 0] \n",
    "\n",
    "    ax[0].imshow(np.uint8(img))\n",
    "    ax[1].imshow(np.uint8(target_img))\n",
    "    ax[2].imshow(np.uint8(out_img))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# output last model\n",
    "print(\"Making prediction with latest model...\")\n",
    "loss, metrics = model.evaluate(valid_dataset, verbose=2, steps=len(valid))\n",
    "\n",
    "iou = \"{:5.2f}\".format(100 * metrics)\n",
    "loss = \"{:5.2f}\".format(loss)\n",
    "\n",
    "print(\"Last model expected results: loss:{}, iou: {}%\".format(loss, iou))\n",
    "res = output_predictions(model, test_dir)\n",
    "\n",
    "name = '0latest_iou' + iou + '_loss' + loss +'.json'\n",
    "with open(os.path.join(output_dir, name), 'w') as fp:\n",
    "        json.dump(res, fp)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# predict with checkpoint models\n",
    "print(\"Making prediction with best models, this may take a while...\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "num = 0\n",
    "ckps = os.listdir(checkpoint_dir)\n",
    "for cp in ckps:\n",
    "    model.load_weights(os.path.join(checkpoint_dir, cp))\n",
    "\n",
    "    loss, metrics = model.evaluate(valid_dataset, verbose=2, steps=len(valid))\n",
    "\n",
    "    #acc = \"{:5.2f}\".format(100 * metrics)\n",
    "    iou = \"{:5.2f}\".format(100 * metrics)\n",
    "    loss = \"{:5.2f}\".format(loss)\n",
    "\n",
    "    print(\"Model expected results: loss:{}, iou: {}%\".format(loss, acc, iou))\n",
    "    res = output_predictions(model, test_dir, use_crop)\n",
    "\n",
    "    # write to file\n",
    "   \n",
    "    name = 'iou' + iou + '_loss' + loss +'.json'\n",
    "    with open(os.path.join(output_dir, name), 'w') as fp:\n",
    "        json.dump(res, fp)\n",
    "\n",
    "    num += 1\n",
    "    print(\"Progress: \"str(num)+\"/\"+str(len(ckps)))\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ]
}