{"cells":[{"metadata":{"trusted":true,"gather":{"logged":1607728548889}},"cell_type":"code","source":"# import\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\nimport os\nfrom PIL import Image\nimport shutil\nfrom glob import iglob\n\n# costants\nSEED = 0xDED\n\ntarget_w = 512\ntarget_h = 512\n\n\nnum_classes = 3    ","execution_count":null,"outputs":[]},{"metadata":{"gather":{"logged":1607728549371},"trusted":true},"cell_type":"code","source":"# usefull functions\n# Print iterations progress\ndef printProgressBar (iteration, total, prefix = 'Progress', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n    filledLength = int(length * iteration // total)\n    bar = fill * filledLength + '-' * (length - filledLength)\n    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n    # Print New Line on Complete\n    if iteration == total: \n        print()\n\ndef fromCwd(path):\n    return os.path.join(os.getcwd(), path)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"gather":{"logged":1607728549641}},"cell_type":"code","source":"# intialization and common utilities\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\ndata_dir = \"/kaggle/input/contadino-2/Development_Dataset/Training\"\ntemp_dir = \"/kaggle/temp/mvdir\"\ncheckpoint_dir = \"/kaggle/temp/checkpoints\"\ntest_dir = \"/kaggle/input/contadino-2/Development_Dataset/Test_Final\"\noutput_dir = \"/kaggle/working\"\n","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1607728549851},"trusted":true},"cell_type":"code","source":"if tf.test.gpu_device_name(): \n    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\nelse:\n   print(\"RIP no gpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"gather":{"logged":1607728771415}},"cell_type":"code","source":"\n#convert a mask in rgb to actual classes\ndef rgb_to_class(mask_arr):\n    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n\n    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n    new_mask_arr[np.where(np.all(mask_arr == [216, 124, 18], axis=-1))] = 0\n    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n\n    return new_mask_arr  \n    \n    \ndef process_image(pil_img):\n    img = pil_img.resize([target_w, target_h])\n    img = np.array(img) / 255.\n\n    return img\n\n\ndef process_mask(pil_img):\n    mask = pil_img.resize([target_w, target_h], resample=Image.NEAREST)\n    mask = np.array(mask)\n    mask = rgb_to_class(mask)\n    mask = np.expand_dims(mask, -1)\n\n    return mask\n\n\ndef shuffle(images, masks):\n    \n    \n    for i in range(len(images)):\n        idx = np.random.randint(0, len(images))\n        # swap an keep track of old place\n        temp_i = images[idx]\n        temp_m = masks[idx]\n        \n        images[idx] = images[i]\n        masks[idx] = masks[i]\n        \n        images[i] = temp_i\n        masks[i] = temp_m\n\n# move all data to images or mask folders into dest dir\ndef loadAllData(source_dir, valid_percent):\n    x_train = []\n    y_train = []\n    x_valid = []\n    y_valid = []\n\n    for folder in iglob(source_dir+'/*/*'):\n\n        # here we got Images in subfolder\n        im_dir = os.path.join(folder, \"Images\")\n        ms_dir = os.path.join(folder, \"Masks\")\n        # read all images and masks\n        images = np.sort(os.listdir(im_dir))\n        masks = np.sort(os.listdir(ms_dir))\n\n        # make validation\n        val_sz = int(len(images) * valid_percent) + 1\n        for i in range(val_sz):\n            idx = np.random.randint(0, len(images))\n\n            if os.path.splitext(images[idx])[0] != os.path.splitext(masks[idx])[0]:\n                print(\"Image-Mask mismatch for: \" +images[idx]+\" -> \"+ masks[idx])\n\n            x_valid.append(process_image(Image.open(os.path.join(im_dir, images[idx]))))\n            y_valid.append(process_mask(Image.open(os.path.join(ms_dir, masks[idx]))))\n\n            sp = folder.split(\"/\")\n            printProgressBar(i+1, val_sz, prefix=str(i+1)+\"/\"+str(val_sz), suffix=\"Valid: \"+sp[-2]+\" - \"+ sp[-1])\n\n            # update refreshed folders without moved elements\n            images = np.delete(images, idx)\n            masks = np.delete(masks, idx)\n        \n        # train data\n        for idx in range(len(images)):\n            if os.path.splitext(images[idx])[0] != os.path.splitext(masks[idx])[0]:\n                print(\"Image-Mask mismatch for: \" +self.images[index]+\" -> \"+ self.masks[index])\n\n            x_train.append(process_image(Image.open(os.path.join(im_dir, images[idx]))))\n            y_train.append(process_mask(Image.open(os.path.join(ms_dir, masks[idx]))))\n\n            sp = folder.split(\"/\")\n            printProgressBar(idx+1, len(images), prefix=str(idx+1)+\"/\"+str(len(images)), suffix=\"Train: \" + sp[-2]+\" - \"+ sp[-1])\n\n    return np.array(x_train), np.array(y_train), np.array(x_valid), np.array(y_valid)\n\n#end    \n\n\nx_train, y_train, x_valid, y_valid = loadAllData(data_dir, 0.1)\n\n#shuffle(x_train, y_train)\n#shuffle(x_valid, y_valid)\n\nprint(\"Train images: \"+ str(len(x_train)))\nprint(\"Validation images: \"+ str(len(x_valid)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"gather":{"logged":1607728773349}},"cell_type":"code","source":"import time\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfor k in range(2):\n  # plot\n  fig, ax = plt.subplots(1, 2)\n  fig.set_size_inches(18, 5, forward=True)\n  \n  idx = np.random.randint(0, len(x_train))\n  augmented_img = x_train[idx]\n  target = y_train[idx]\n\n  augmented_img = augmented_img * 255  # denormalize\n\n  target_img = np.zeros([target.shape[0], target.shape[1], 3])\n  target = tf.squeeze(target, axis=-1)\n  #target = np.array(target).flatten()\n\n  print(np.unique(target))\n\n\n  target_img[np.where(target == 0)] = [0, 0, 0]\n  target_img[np.where(target == 1)] = [0, 255, 0] \n  target_img[np.where(target == 2)] = [255, 0, 0] \n\n  ax[0].imshow(np.uint8(augmented_img))\n  ax[1].imshow(np.uint8(target_img))\n\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"tags":[],"gather":{"logged":1607728773623}},"cell_type":"code","source":"\nvgg = keras.applications.VGG16(input_shape=[target_h, target_w, 3], include_top=False)\nfor l in vgg.layers[:int(len(vgg.layers) * 0.2)]:\n    l.trainable = False\n\nvgg.summary()\n\ndef up_block(down_in, prev, z_sz):\n    prev = keras.layers.UpSampling2D()(prev)\n    prev = keras.layers.Concatenate(axis=-1)([down_in, prev])\n    prev = keras.layers.Conv2D(z_sz,3, activation = 'relu', padding = 'same')(prev)\n    return keras.layers.Conv2D(z_sz,3, activation = 'relu', padding = 'same')(prev)\n\nmult = 1\n\nlast = up_block(vgg.layers[-6].output, vgg.layers[-2].output, 256 * mult)\nlast = up_block(vgg.layers[-10].output, last, 128 * mult)\nlast = up_block(vgg.layers[-14].output, last, 64 * mult)\nlast = up_block(vgg.layers[-17].output, last, 32 * mult)\n\nlast = keras.layers.Conv2D(16 * mult, 3, activation = 'relu', padding = 'same')(last)\nlast = keras.layers.Conv2D(16 * mult, 3, activation = 'relu', padding = 'same')(last)\n\nlast = keras.layers.Conv2D(num_classes, 1, activation = 'softmax')(last)\n\nmodel = keras.Model(vgg.input, last)\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"gather":{"logged":1607728773857}},"cell_type":"code","source":"import tensorflow.keras.backend as K\n\nlearning_rate = 1e-5\n\ndef weightedLoss(originalLossFunc, weightsList):\n\n    def lossFunc(true, pred):\n\n        axis = -1 #if channels last \n        #axis=  1 #if channels first\n\n\n        #argmax returns the index of the element with the greatest value\n        #done in the class axis, it returns the class index    \n        classSelectors = K.argmax(true, axis=axis) \n            #if your loss is sparse, use only true as classSelectors\n\n        #considering weights are ordered by class, for each class\n        #true(1) if the class index is equal to the weight index   \n        classSelectors = [K.equal(np.int64(i), classSelectors) for i in range(len(weightsList))]\n\n        #casting boolean to float for calculations  \n        #each tensor in the list contains 1 where ground true class is equal to its index \n        #if you sum all these, you will get a tensor full of ones. \n        classSelectors = [K.cast(x, K.floatx()) for x in classSelectors]\n\n        #for each of the selections above, multiply their respective weight\n        weights = [sel * w for sel,w in zip(classSelectors, weightsList)] \n\n        #sums all the selections\n        #result is a tensor with the respective weight for each element in predictions\n        weightMultiplier = weights[0]\n        for i in range(1, len(weights)):\n            weightMultiplier = weightMultiplier + weights[i]\n\n\n        #make sure your originalLossFunc only collapses the class axis\n        #you need the other axes intact to multiply the weights tensor\n        loss = originalLossFunc(true,pred) \n        loss = loss * weightMultiplier\n\n        return loss\n    return lossFunc\n\n\ndef meanIoU(y_true, y_pred):\n    # get predicted class from softmax\n    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n\n    per_class_iou = []\n\n    for i in range(1,3): # exclude the background class 0\n      # Get prediction and target related to only a single class (i)\n      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n      intersection = tf.reduce_sum(class_true * class_pred)\n      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n    \n      iou = (intersection + 1e-7) / (union + 1e-7)\n      per_class_iou.append(iou)\n\n    return tf.reduce_mean(per_class_iou)\n\n\n# compole model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n    loss=weightedLoss(keras.losses.SparseCategoricalCrossentropy(),[1,1,5]), \n    metrics=[meanIoU] \n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"gather":{"logged":1607728774129}},"cell_type":"code","source":"from datetime import datetime\n\nnow = datetime.now().strftime('%b%d_%H-%M-%S')\n\n#checkpoint_dir = checkpoint_dir+str(now)\n\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# setup callbacks\n# -----------------------\n\nclass ShuffleCallback(keras.callbacks.Callback):\n\n    def on_epoch_end(self, epoch, logs=None):\n        shuffle(x_train, y_train)\n\n\n\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(filepath=os.path.join(checkpoint_dir, 'cp_{epoch:02d}.ckpt'), save_best_only=True, monitor='val_loss'),\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n    #ShuffleCallback()\n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1\nepochs = 30\n\nhistory = model.fit(\n          x=x_train,\n          y=y_train,\n          epochs=epochs,\n          batch_size=batch_size,\n          validation_data=(x_valid, y_valid),\n          callbacks=callbacks\n          )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot alla metrics \nidx = 1\nmtr = ['loss','meanIoU']\n\nplt.figure(figsize=(25, 8))\n\nfor m in mtr:\n    x = history.history[m]\n    val_x = history.history['val_' + m]\n\n    plt.subplot(1, len(mtr), idx)\n    plt.plot(x, label='Training ' + m)\n    plt.plot(val_x, label='Validation ' + m)\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation ' + m)\n    idx += 1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_encode(img):\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n# return {rle crop prediction, rle weed prediction} img shape\ndef make_prediction(model, image):\n\n    #print(\"Original size: \"+ str(image.size))\n\n    img = image.resize([target_w, target_h]) # resize to the network expected size and keep a copy of the original img\n    img = np.array(img) / 255. # rescale\n    img = tf.expand_dims(img, 0) # add \"batch\" dimension\n\n    out = model(img, training=False) # predict\n\n    out= tf.squeeze(out, 0) # remove batch dimension to get a 3d vector\n    # we got a 3d vector with 3 deph and we need to rescale it to original input size\n    # so we denormalize if (output of softmax is 0 < val < 1)\n    # and we treat it as img to resize in plt\n    out = Image.fromarray(np.uint8(out * 255.)).resize(image.size)\n    #print(\"Predictions:\" + str(out.size))\n    out = np.array(out)\n    # rescale and resize should preserve the old max channels\n    # so we can ignore the rescaling to 0-1 again\n    out = tf.math.argmax(out, axis=-1) \n\n    \n\n    return out, image.size\n\n# create function to evaluate the model and output results in requeisted format\ndef output_predictions(model, test_dir):\n    submission_dict = {}\n    #print(test_dir)\n\n    # iterate all datasets keeping name\n    for team in os.listdir(test_dir): #team == dataset name\n        team_dir = os.path.join(test_dir, team)\n\n        # iterate all corps for a dataset keeping name\n        for crop in os.listdir(team_dir):\n            crop_dict = {}\n            img_dir = os.path.join(team_dir, crop)\n            img_dir = os.path.join(img_dir, \"Images\")\n            # iterate all images \n            files = os.listdir(img_dir)\n            i = 0\n            for fl in files:\n                #print(\"img: \"+ fl)\n                # get prediction from model\n                image = Image.open(os.path.join(img_dir, fl))\n                prediction, shape = make_prediction(model, image)\n                \n                prediction = np.array(prediction)\n\n                filename, file_extension = os.path.splitext(fl)\n                # prepare output dict\n                submission_dict[filename] = {\n                    'shape': shape,\n                    'team': team,\n                    'crop': crop,\n                    'segmentation': {'crop': rle_encode(prediction == 1), 'weed':rle_encode(prediction == 2)}\n                }\n\n                i += 1\n                printProgressBar(i, len(files), prefix=\"Prediction\", suffix=team+\" - \"+crop)\n            # end for fl\n        # end for crop\n    #end for team\n\n    return submission_dict\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot result of a validation image to understand if net is learning right\nsamples = 3\n\nfor i in range(samples):\n    idx = np.random.randint(0, len(x_valid))\n    img = x_valid[idx]\n    mask = y_valid[idx]\n\n    fig, ax = plt.subplots(1, 3)\n    fig.set_size_inches(18, 5, forward=True)\n\n    img = np.array(img) * 255\n    i = Image.fromarray(np.uint8(img))\n    out, shape = make_prediction(model, i)\n\n    # expected mask\n\n    target_img = np.zeros([target.shape[0], target.shape[1], 3])\n    target = tf.squeeze(mask, axis=-1)\n\n    target_img[np.where(target == 0)] = [0, 0, 0]\n    target_img[np.where(target == 1)] = [0, 255, 0] \n    target_img[np.where(target == 2)] = [255, 0, 0] \n\n    #prediction\n    out_img = np.zeros([out.shape[0], out.shape[1], 3])\n\n    out_img[np.where(out == 0)] = [0, 0, 0]\n    out_img[np.where(out == 1)] = [0, 255, 0] \n    out_img[np.where(out == 2)] = [255, 0, 0] \n\n    ax[0].imshow(np.uint8(img))\n    ax[1].imshow(np.uint8(target_img))\n    ax[2].imshow(np.uint8(out_img))\n\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# output last model\nprint(\"Making prediction with latest model...\")\nloss, metrics = model.evaluate(x=x_valid, y=y_valid, verbose=2, steps=len(x_valid))\n\niou = \"{:5.2f}\".format(100 * metrics)\nloss = \"{:5.2f}\".format(loss)\n\nprint(\"Last model expected results: loss:{}, iou: {}%\".format(loss, iou))\nres = output_predictions(model, test_dir)\n\nname = '0latest_iou' + iou + '_loss' + loss +'.json'\nwith open(os.path.join(output_dir, name), 'w') as fp:\n        json.dump(res, fp)\n\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls /kaggle/temp/checkpoints","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\n# predict with checkpoint models\nprint(\"Making prediction with best models, this may take a while...\")\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nnum = 0\nckps = os.listdir(checkpoint_dir)\nckps = ['cp_20.ckpt']\n\nfor cp in ckps:\n    path_to_model = os.path.join(checkpoint_dir, cp)\n    m2 = tf.keras.models.load_model(path_to_model, custom_objects={'meanIoU':meanIoU})\n    \n\n    loss, metrics = m2.evaluate(x=x_valid, y=y_valid, verbose=2, steps=len(x_valid))\n\n    #acc = \"{:5.2f}\".format(100 * metrics)\n    iou = \"{:5.2f}\".format(100 * metrics)\n    loss = \"{:5.2f}\".format(loss)\n\n    print(\"Model expected results: loss:{}, iou: {}%\".format(loss, iou))\n    res = output_predictions(model, test_dir)\n\n    # write to file\n   \n    name = 'iou' + iou + '_loss' + loss +'.json'\n    with open(os.path.join(output_dir, name), 'w') as fp:\n        json.dump(res, fp)\n\n    num += 1\n    print(\"Progress: \"+str(num)+\"/\"+str(len(ckps)))\n\nprint(\"Done\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}