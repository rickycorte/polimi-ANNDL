{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlX3sjDFxAgI5TE7HBSKkn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anacleto98/ANNDL_projects/blob/master/ANNDL_homework3/Homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9QX16bHpU77"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np \r\n",
        "import os\r\n",
        "\r\n",
        "SEED = 1234\r\n",
        "\r\n",
        "tf.random.set_seed(SEED)\r\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylHhlMwwqjCM",
        "outputId": "95629b16-34bc-4f18-c501-df15bbf1e659"
      },
      "source": [
        "working_directory = 'D:\\\\ANNDL_Challenge3_Dataset'\r\n",
        "dataset_dir = os.path.join(working_directory,'VQA_Dataset')\r\n",
        "\r\n",
        "print(os.listdir(dataset_dir))\r\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Images', 'test_questions.json', 'train_questions_annotations.json']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a3fr6VrslRU",
        "outputId": "12afed28-6452-442d-83a0-a4af40dd1f0e"
      },
      "source": [
        "# importing json as python dictionaries \r\n",
        "\r\n",
        "import json \r\n",
        "\r\n",
        "with open(os.path.join(dataset_dir,'test_questions.json')) as json_file:\r\n",
        "  test_question = json.load(json_file)\r\n",
        "\r\n",
        "with open(os.path.join(dataset_dir,'train_questions_annotations.json')) as json_file:\r\n",
        "  train_questions_annotations = json.load(json_file)\r\n",
        "\r\n",
        "image_labels_ = os.listdir(os.path.join(dataset_dir,'Images'))\r\n",
        "image_labels = []\r\n",
        "\r\n",
        "for name in image_labels_:\r\n",
        "  image_labels.append(name[:-4])\r\n",
        "\r\n",
        "\r\n",
        "print(test_question[image_labels[2]])\r\n",
        "print(image_labels[2])\r\n",
        "\r\n",
        "questions_set = set()\r\n",
        "answers_set = set()\r\n",
        "answers_set_train = set()\r\n",
        "\r\n",
        "for key in train_questions_annotations:\r\n",
        "  questions_set.add(train_questions_annotations[key]['question'] + ' <eos>')\r\n",
        "  answers_set.add(train_questions_annotations[key]['answer'] + ' <eos>')\r\n",
        "  answers_set_train.add('<sos> ' + train_questions_annotations[key]['answer'])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'question': 'Is the woman on the couch sporting white hair?', 'image_id': '1'}\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfxMIWvXujBK",
        "outputId": "7a56c701-0b23-47cf-c808-c571aa58270e"
      },
      "source": [
        "# number of answers \r\n",
        "\r\n",
        "num_answers = len(answers_set)\r\n",
        "\r\n",
        "# longest question is 100 chars\r\n",
        "max_string = max(questions_set, key=len)\r\n",
        "num_words = len(max_string.split(' '))\r\n",
        "\r\n",
        "num_answers # number of classes \r\n",
        "\r\n",
        "words = set()\r\n",
        "for question in questions_set:\r\n",
        "  for w in question.split(' '):\r\n",
        "    words.add(w)\r\n",
        "\r\n",
        "print(len(words))\r\n",
        "\r\n",
        "MAX_NUM_WORDS = 10000\r\n",
        "\r\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xwSDeUPzYPK",
        "outputId": "e1a36b4e-ce58-4e66-c760-cb8d2798f1e9"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "# dictionary of training:  train_questions_annotations\r\n",
        "# structure: \r\n",
        "questions_tokenizer = Tokenizer(num_words = MAX_NUM_WORDS,filters = '?')\r\n",
        "answers_tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\r\n",
        "\r\n",
        "questions_tokenizer.fit_on_texts(questions_set)\r\n",
        "answers_tokenizer.fit_on_texts(answers_set.union(answers_set_train))\r\n",
        "\r\n",
        "questions_tokenized = questions_tokenizer.texts_to_sequences(questions_set)\r\n",
        "answers_tokenized = answers_tokenizer.texts_to_sequences(answers_set)\r\n",
        "answers_tokenized_train = answers_tokenizer.texts_to_sequences(answers_set_train)\r\n",
        "\r\n",
        "\r\n",
        "# all sentences need a padding\r\n",
        "\r\n",
        "max_length_question = max(len(sentence) for sentence in questions_tokenized)\r\n",
        "max_length_answer_train = max(len(sentence) for sentence in answers_tokenized_train)\r\n",
        "max_length_answer = max(len(sentence) for sentence in answers_tokenized)    \r\n",
        "\r\n",
        "print(max_length_question)\r\n",
        "print(max_length_answer)\r\n",
        "print(max_length_answer_train)\r\n",
        "\r\n",
        "# pad sequences\r\n",
        "question_encoder_inputs = pad_sequences(questions_tokenized, maxlen=max_length_question)\r\n",
        "answer_encoder_inputs = pad_sequences(answers_tokenized_train,maxlen=max_length_answer_train)\r\n",
        "answer_outputs = pad_sequences(answers_tokenized,maxlen=max_length_answer,padding='post')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL2tAmJP9mJz"
      },
      "source": [
        "encoded_train_dictionary = {}\r\n",
        "\r\n",
        "for key in train_questions_annotations:\r\n",
        "  encoded_train_dictionary[key] = {}\r\n",
        "  encoded_train_dictionary[key]['question'] = questions_tokenizer.texts_to_sequences(train_questions_annotations[key]['question']) \r\n",
        "  encoded_train_dictionary[key]['image_id'] = train_questions_annotations[key]['image_id']\r\n",
        "  encoded_train_dictionary[key]['answer']   = answers_tokenizer.texts_to_sequences(train_questions_annotations[key]['answer'] + ' <eos>')\r\n",
        "  encoded_train_dictionary[key]['answer_train'] = answers_tokenizer.texts_to_sequences('<sos> ' + train_questions_annotations[key]['answer'])\r\n",
        "\r\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZMHxBWJJN79"
      },
      "source": [
        "# not right\r\n",
        "\r\n",
        "with open(os.path.join(working_directory,'encoded.json'), 'w') as fp:\r\n",
        "    json.dump(encoded_train_dictionary, fp)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SHSt9q6L0OT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}